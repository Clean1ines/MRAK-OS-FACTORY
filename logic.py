from dotenv import load_dotenv
import os
import re
import httpx
import asyncio
from groq import Groq
from typing import Optional, Dict, Any

import db

load_dotenv()


class MrakOrchestrator:
    def __init__(self, api_key=None):
        self.api_key = api_key or os.getenv("GROQ_API_KEY")
        self.gh_token = os.getenv("GITHUB_TOKEN")
        self.client = Groq(
            api_key=self.api_key, http_client=httpx.Client(timeout=120.0)
        )
        self.base_url = "https://api.groq.com/openai/v1"

        # –ú–∞–ø–ø–∏–Ω–≥ —Ä–µ–∂–∏–º–æ–≤ –Ω–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è —Å URL
        self.mode_map = {
            "01_CORE": "SYSTEM_PROMPT_URL",
            "02_UI_UX": "GH_URL_UI_UX",
            "03_SOFT_ENG": "GH_URL_SOFT_ENG",
            "04_FAILURE": "GH_URL_FAILURE",
            "06_TRANSLATOR": "GH_URL_TRANSLATOR",
            "07_INTEGRATION_PLAN": "GH_URL_INTEGRATION_PLAN",
            "08_PROMPT_COUNCIL": "GH_URL_PROMPT_COUNCIL",
            "09_ALGO_COUNCIL": "GH_URL_ALGO_COUNCIL",
            "10_FULL_CODE_GEN": "GH_URL_FULL_CODE_GEN",
            "11_REQ_COUNCIL": "GH_URL_REQ_COUNCIL",
            "12_SELF_ANALYSIS_FACTORY": "GH_URL_SELF_ANALYSIS_FACTORY",
            "13_ARTIFACT_OUTPUT": "GH_URL_MPROMPT",
            "14_PRODUCT_COUNCIL": "GH_URL_PRODUCT_COUNCIL",
            "15_BUSINESS_REQ_GEN": "GH_URL_BUSINESS_REQ_GEN",   # –ù–æ–≤—ã–π —Ä–µ–∂–∏–º
            "16_REQ_ENG_COUNCIL": "GH_URL_REQ_ENG_COUNCIL",     # –ù–æ–≤—ã–π —Ä–µ–∂–∏–º
        }

        # –ú–∞–ø–ø–∏–Ω–≥ —Ç–∏–ø–∞ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–∞ –Ω–∞ —Ä–µ–∂–∏–º –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        self.type_to_mode = {
            "BusinessIdea": "14_PRODUCT_COUNCIL",          # –∏–¥–µ—è -> —Å–æ–≤–µ—Ç —Ç–∏—Ç–∞–Ω–æ–≤
            "ProductCouncilAnalysis": None,                 # –∞–Ω–∞–ª–∏–∑ –Ω–µ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç—Å—è (—ç—Ç–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç)
            "BusinessRequirement": "15_BUSINESS_REQ_GEN",   # –∞–Ω–∞–ª–∏–∑ -> –±–∏–∑–Ω–µ—Å-—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è
            "ReqEngineeringAnalysis": "16_REQ_ENG_COUNCIL", # –±–∏–∑–Ω–µ—Å-—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è -> –∞–Ω–∞–ª–∏–∑ –∏–Ω–∂–µ–Ω–µ—Ä–∏–∏
            "FunctionalRequirement": None,                  # –º–æ–∂–µ—Ç –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å—Å—è –æ—Ç–¥–µ–ª—å–Ω–æ
            "CodeArtifact": "10_FULL_CODE_GEN",             # —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è -> –∫–æ–¥
        }

    def get_active_models(self):
        fallback_models = [
            {"id": "openai/gpt-oss-120b"},
            {"id": "llama-3.3-70b-versatile"},
            {"id": "llama-3.1-8b-instant"},
            {"id": "groq/compound"},
            {"id": "qwen/qwen3-32b"},
        ]
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
        }
        try:
            with httpx.Client() as client:
                response = client.get(
                    f"{self.base_url}/models", headers=headers, timeout=10.0
                )
                if response.status_code == 200:
                    data = response.json()
                    active = [
                        {"id": m["id"]}
                        for m in data.get("data", [])
                        if "whisper" not in m["id"]
                    ]
                    return active if active else fallback_models
                return fallback_models
        except Exception:
            return fallback_models

    async def get_system_prompt(self, mode: str):
        if mode == "07_BYPASS":
            return "You are a helpful assistant."

        env_var = self.mode_map.get(mode, "SYSTEM_PROMPT_URL")
        url = os.getenv(env_var)

        if not url:
            return f"System Error: URL for mode {mode} not found in environment."

        headers = {
            "Accept": "application/vnd.github.v3.raw",
        }
        if self.gh_token:
            headers["Authorization"] = f"token {self.gh_token}"

        async with httpx.AsyncClient() as c:
            try:
                r = await c.get(url, headers=headers, timeout=15)
                if r.status_code == 200:
                    return r.text.strip()
                return f"Error fetching prompt: {r.status_code} for mode {mode}"
            except Exception as e:
                return f"Connection Error: {str(e)}"

    def _pii_filter(self, text: str) -> str:
        text = re.sub(r"[\w\.-]+@[\w\.-]+\.\w+", "[EMAIL_REDACTED]", text)
        text = re.sub(r"(gsk_|sk-)[a-zA-Z0-9]{20,}", "[KEY_REDACTED]", text)
        return text

    async def generate_artifact(
        self,
        artifact_type: str,
        user_input: str,
        parent_artifact: Optional[Dict[str, Any]] = None,
        model_id: Optional[str] = None,
        project_id: Optional[str] = None
    ) -> Optional[str]:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∞—Ä—Ç–µ—Ñ–∞–∫—Ç —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ —Ç–∏–ø–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–æ–≥–æ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–∞.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç ID —Å–æ–∑–¥–∞–Ω–Ω–æ–≥–æ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–∞ –∏–ª–∏ None.
        """
        mode = self.type_to_mode.get(artifact_type)
        if not mode:
            raise ValueError(f"No generation mode defined for artifact type {artifact_type}")

        # –ü–æ–ª—É—á–∞–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç
        sys_prompt = await self.get_system_prompt(mode)
        if sys_prompt.startswith("Error") or sys_prompt.startswith("System Error"):
            raise Exception(f"Failed to get system prompt: {sys_prompt}")

        # –§–æ—Ä–º–∏—Ä—É–µ–º –≤—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç –¥–ª—è LLM
        if parent_artifact:
            # –ï—Å–ª–∏ –µ—Å—Ç—å —Ä–æ–¥–∏—Ç–µ–ª—å, –ø–µ—Ä–µ–¥–∞—ë–º –µ–≥–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –∫–∞–∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç
            prompt = f"Parent artifact ({parent_artifact['type']}):\n{json.dumps(parent_artifact['content'])}\n\nUser input:\n{user_input}"
        else:
            prompt = user_input

        # –í—ã–∑—ã–≤–∞–µ–º LLM –∏ –ø–æ–ª—É—á–∞–µ–º –ø–æ–ª–Ω—ã–π –æ—Ç–≤–µ—Ç (–Ω–µ –ø–æ—Ç–æ–∫–æ–≤—ã–π)
        try:
            response = self.client.chat.completions.create(
                model=model_id or "llama-3.3-70b-versatile",
                messages=[
                    {"role": "system", "content": sys_prompt},
                    {"role": "user", "content": self._pii_filter(prompt)},
                ],
                temperature=0.6,
            )
            result_text = response.choices[0].message.content
        except Exception as e:
            raise Exception(f"LLM call failed: {e}")

        # –ü—ã—Ç–∞–µ–º—Å—è —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å JSON, –µ—Å–ª–∏ –æ–∂–∏–¥–∞–µ—Ç—Å—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç
        try:
            # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ JSON –≤ –æ—Ç–≤–µ—Ç–µ (–µ—Å–ª–∏ –ø—Ä–æ–º–ø—Ç –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç JSON)
            # –í –ø—Ä–æ—Å—Ç–æ–º —Å–ª—É—á–∞–µ —Å—á–∏—Ç–∞–µ–º, —á—Ç–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç ‚Äî —ç—Ç–æ –∏ –µ—Å—Ç—å JSON
            result_data = json.loads(result_text)
        except json.JSONDecodeError:
            # –ï—Å–ª–∏ –Ω–µ JSON, —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ —Ç–µ–∫—Å—Ç
            result_data = {"text": result_text}

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∫–∞–∫ –Ω–æ–≤—ã–π –∞—Ä—Ç–µ—Ñ–∞–∫—Ç
        artifact_id = await db.save_artifact(
            artifact_type=artifact_type,
            content=result_data,
            owner="system",
            status="GENERATED",
            project_id=project_id,
            parent_id=parent_artifact['id'] if parent_artifact else None
        )
        return artifact_id

    async def stream_analysis(self, user_input: str, system_prompt: str, model_id: str, mode: str, project_id: Optional[str] = None):
        """(–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)"""
        clean_input = self._pii_filter(user_input)
        full_response = ""
        try:
            raw_res = self.client.chat.completions.with_raw_response.create(
                model=model_id,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": clean_input},
                ],
                stream=True,
                temperature=0.6,
            )

            rt = raw_res.headers.get("x-ratelimit-remaining-tokens", "---")
            rr = raw_res.headers.get("x-ratelimit-remaining-requests", "---")
            yield f"__METADATA__{rt}|{rr}__"

            for chunk in raw_res.parse():
                if chunk.choices and chunk.choices[0].delta.content:
                    content = chunk.choices[0].delta.content
                    full_response += content
                    yield content

            if full_response:
                artifact_data = {
                    "user_input": clean_input,
                    "system_prompt": system_prompt[:500] + ("..." if len(system_prompt) > 500 else ""),
                    "model": model_id,
                    "mode": mode,
                    "response": full_response,
                    "metadata": {
                        "tokens_remaining": rt,
                        "requests_remaining": rr
                    }
                }
                asyncio.create_task(
                    db.save_artifact(
                        artifact_type="LLMResponse",
                        content=artifact_data,
                        owner="system",
                        status="GENERATED",
                        project_id=project_id
                    )
                )
        except Exception as e:
            err_msg = str(e).lower()
            if "403" in err_msg:
                yield "üî¥ **ACCESS_DENIED**: API –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω –≤ –≤–∞—à–µ–º —Ä–µ–≥–∏–æ–Ω–µ."
            elif "429" in err_msg:
                yield "üî¥ **RATE_LIMIT**: –ü–æ–¥–æ–∂–¥–∏—Ç–µ, –ª–∏–º–∏—Ç—ã –∏—Å—á–µ—Ä–ø–∞–Ω—ã."
            else:
                yield f"üî¥ **GROQ_API_ERROR**: {str(e)}"
